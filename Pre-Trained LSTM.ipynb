{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a Pre-Trained Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to train an LSTM network, let's see try seeing the results on our own input text!\n",
    "\n",
    "First, we declare some of our hyperparamters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll load in our data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDimensions = 300\n",
    "maxSeqLength = 250\n",
    "batchSize = 50\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy').tolist()\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our graph. This has to be the same code used to create the model that was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input') as scope:\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses], name='input_labels')\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength], name='input_data')\n",
    "\n",
    "with tf.name_scope('word2vec'):\n",
    "    data = tf.nn.embedding_lookup(wordVectors,input_data, name='word2vec')\n",
    "\n",
    "with tf.name_scope('sentiment_network'):\n",
    "    c1= tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    c1 = tf.nn.rnn_cell.DropoutWrapper(cell=c1)\n",
    "    c2 = tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    c2 = tf.nn.rnn_cell.DropoutWrapper(cell=c2)\n",
    "    lstmCell = tf.nn.rnn_cell.MultiRNNCell([c1, c2], state_is_tuple=True)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]), name='Weights')\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]), name='Biases')\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels), name='loss')\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss, name='AdamOptimizer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models/20180203-222319-50-64'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we input our own text, let's first define a couple of functions. The first is a function to make sure the sentence is in the proper format, and the second is a function that obtains the word vectors for each of the words in a given sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create our simple predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    predictedSentiment = sess.run(prediction, {input_data: getSentenceMatrix(sentence)})[0]\n",
    "    high_positive = high_negative = False\n",
    "    if (predictedSentiment[0] > 2):\n",
    "        high_positive = True\n",
    "    if (predictedSentiment[1] > 2):\n",
    "        high_negative = True\n",
    "    if high_positive and high_negative:\n",
    "        return 0\n",
    "    elif high_positive:\n",
    "        return 1\n",
    "    elif high_negative:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Great wonderful time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"That horrible show was awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the fun stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_happy_face():\n",
    "    print(u'\\U0001f604')\n",
    "def print_neutral_face():\n",
    "    print(u'\\U0001f610')\n",
    "def print_sad_face():\n",
    "    print(u'\\U00002639')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        # Skip if lang is not english\n",
    "        if status.lang != 'en':\n",
    "            return\n",
    "        print(status.text)\n",
    "        print('-------------------')\n",
    "        prediction = predict(status.text)\n",
    "        if prediction == 1:\n",
    "            print_happy_face()\n",
    "        elif prediction == 0:\n",
    "            print_neutral_face()\n",
    "        else:\n",
    "            print_sad_face()\n",
    "        print('-------------------')    \n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            #returning False in on_data disconnects the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myStream.filter(track=['bitcoin'], async=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myStream.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "from os import system\n",
    "\n",
    "# Windows\n",
    "# engine = pyttsx3.init()\n",
    "def sayResponseToPrediction(prediction):\n",
    "    if prediction == 1:\n",
    "        stmt = \"Happy that you're happy\"\n",
    "    elif prediction == 0:\n",
    "        stmt = \"Hmm, I have no feelings towards that either\"\n",
    "    else:\n",
    "        stmt = \"Relax, relax, relax\"\n",
    "    \n",
    "    # osX\n",
    "    system('say %s' % stmt)\n",
    "    # Windiows\n",
    "    #engine.say(stmt)\n",
    "    #engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sayResponseToPrediction(predict(\"I'm feeling sad and angry, but there's always hope\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
